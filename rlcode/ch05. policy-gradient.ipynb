{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 개요\n",
    "\n",
    "정책 기반 강화학습의 목표\n",
    "$$\n",
    "\\text{maximize } J(\\theta)\n",
    "$$\n",
    "\n",
    "정책신경망 업데이트\n",
    "$$\n",
    "\\theta_{t+1}=\\theta_t+\\alpha \\nabla_\\theta J(\\theta)\n",
    "$$\n",
    "\n",
    "가치함수로 나타내는 목표함수의 정의\n",
    "$$\n",
    "J(\\theta) = v_{\\pi_\\theta}(s_0)\n",
    "$$\n",
    "\n",
    "목표함수의 미분\n",
    "\\begin{align}\n",
    "\\nabla_\\theta J(\\theta) & = \\nabla_\\theta v_{\\pi_\\theta} (s_0) \\\\\n",
    "\\nabla_\\theta J(\\theta) & = \\sum_s d_{\\pi_\\theta} (s) \\sum_a \\nabla_\\theta \\pi_\\theta (a | s) q_\\pi (s, a) \\\\\n",
    "\\nabla_\\theta J(\\theta) & = \\sum_s d_{\\pi_\\theta} (s) \\sum_a \\pi_\\theta (a | s) \\times \\nabla_\\theta \\log \\pi_\\theta (a|s) q_\\pi (s,a) \\\\ \n",
    "\\nabla_\\theta J(\\theta) & = E_{\\pi_\\theta} [ \\nabla_\\theta \\log \\pi_\\theta (a|s) q_\\pi (s,a) ] \\\\ \n",
    "\\end{align}\n",
    "\n",
    "폴리시 그레디언트 업데이트식\n",
    "\\begin{align}\n",
    "\\theta_{t+1} & = \\theta_t+\\alpha \\nabla_\\theta J(\\theta) \\approx \\theta_t + \\alpha [ \\nabla_\\theta \\log \\pi_\\theta (a|s) q_\\pi (s,a) ] \\\\\n",
    "\\theta_{t+1} & \\approx \\theta_t + \\alpha [ \\nabla_\\theta \\log \\pi_\\theta (a|s) G_t ]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mgym\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEAgent:\n",
    "    def __init__(self, env):\n",
    "        self.action_size = env.action_space.n\n",
    "        self.state_size = 4\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "        self.model = self._build_model()\n",
    "        self.optimizer = self._build_optimizer()\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "        \n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='softmax'))\n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "    def _build_optimizer(self):\n",
    "        action = K.placeholder(shape=[None, 5])\n",
    "        discounted_rewards = K.placeholder(shape=[None,])\n",
    "        action_prob = K.sum(action * self.model.output, axis=1)\n",
    "        cross_entropy = K.log(action_prob) * discounted_rewards\n",
    "        loss = -K.sum(cross_entropy)\n",
    "        \n",
    "        optimizer = Adam(lr=self.learning_rate)\n",
    "        updates = optimizer.get_updates(self.model.trainable_weights, [], loss)\n",
    "        train = K.function([self.model.input, action, discounted_rewards], [], updates=updates)\n",
    "        \n",
    "        return train\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        policy = self.model.predict(state)[0]\n",
    "        return np.random.choice(self.action_size, 1, p=policy)[0], policy\n",
    "    \n",
    "    def discount_rewards(self, rewards):\n",
    "        discounted_rewards = np.zeros_like(rewards)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            running_add = running_add + self.discount_factor + rewards[t]\n",
    "            discounted_rewards[t] = running_add\n",
    "        return discounted_rewards\n",
    "    \n",
    "    def append_sample(self, state, action, reward):\n",
    "        self.states.append(state[0])\n",
    "        self.rewards.append(reward)\n",
    "        act = np.zeros(self.action_size)\n",
    "        act[action] = 1\n",
    "        self.actions.append(act)\n",
    "        \n",
    "    def train_model(self):\n",
    "        # print(self.states)\n",
    "        discounted_rewards = np.float32(self.discount_rewards(self.rewards))\n",
    "        discounted_rewards -= np.mean(discounted_rewards)\n",
    "        discounted_rewards /= np.std(discounted_rewards)\n",
    "        self.states = np.array(self.states)\n",
    "        self.actions = np.array(self.actions)\n",
    "        self.optimizer([self.states, self.actions, discounted_rewards])\n",
    "        self.states, self.actions, self.rewards = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = mgym.make('5x5moving')\n",
    "agent = REINFORCEAgent(env)\n",
    "\n",
    "global_step = 0\n",
    "scores, episodes = [], []\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, -1])\n",
    "    \n",
    "    while not done:\n",
    "        global_step += 1\n",
    "        \n",
    "        action, policy = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, -1])\n",
    "        agent.append_sample(state, action, reward)\n",
    "        \n",
    "#         if global_step % 1000 == 0:\n",
    "#             print(global_step, state, action, next_state, reward, done)\n",
    "        \n",
    "        score += reward\n",
    "        state = copy.deepcopy(next_state)\n",
    "        \n",
    "        if done:\n",
    "            agent.train_model()\n",
    "            \n",
    "            scores.append(score)\n",
    "            episodes.append(e)\n",
    "            score = round(score, 2)\n",
    "            print(e, score, global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
