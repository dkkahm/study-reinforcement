{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque, namedtuple\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPACITY = 10000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.001\n",
    "UPDATE_TARGET_MODEL_INTERVAL = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.epsilon = 1.0\n",
    "        self.min_epsilon = 0.01\n",
    "        self.epsilon_decay = 0.5\n",
    "        \n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.memory = deque(maxlen=CAPACITY)\n",
    "        self.main_model = self._get_model()\n",
    "        self.target_model = self._get_model()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.main_model.cuda()\n",
    "            self.target_model.cuda()\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.main_model.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "    def train(self, state, action, next_state, reward, done, episode=None):\n",
    "        self.memory.append(Transition(state, action, next_state, reward, done))\n",
    "        \n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return None\n",
    "        \n",
    "        transitions = random.sample(self.memory, BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        state_batch = torch.FloatTensor(batch.state).view(-1, self.num_states)\n",
    "        action_batch = torch.LongTensor(batch.action).view(-1, 1)\n",
    "        reward_batch = torch.FloatTensor(batch.reward).view(-1, 1)\n",
    "        next_state_batch = torch.FloatTensor(batch.next_state).view(-1, self.num_states)\n",
    "        done_batch = torch.ByteTensor(batch.done).view(-1, 1)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            state_batch = state_batch.cuda()\n",
    "            action_batch = action_batch.cuda()\n",
    "            reward_batch = reward_batch.cuda()\n",
    "            next_state_batch = next_state_batch.cuda()\n",
    "            done_batch = done_batch.cuda()\n",
    "        \n",
    "        state_action_values = self.main_model(state_batch).gather(1, action_batch)\n",
    "        \n",
    "        next_state_values = self.target_model(next_state_batch).max(1)[0].view(-1, 1)\n",
    "        td_targets = reward_batch + GAMMA * (1 - done_batch) * next_state_values\n",
    "        \n",
    "        loss = F.smooth_l1_loss(state_action_values, td_targets)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if done and episode % UPDATE_TARGET_MODEL_INTERVAL == 0:\n",
    "            self._update_target_model()\n",
    "\n",
    "        return loss.item()\n",
    "        \n",
    "    def get_action(self, state, train=True, episode=None):\n",
    "        if train:\n",
    "            if self.epsilon <= np.random.uniform(0, 1):\n",
    "                with torch.no_grad():\n",
    "                    state = torch.FloatTensor(state).view(1, -1)\n",
    "                    if torch.cuda.is_available():\n",
    "                        state = state.cuda()\n",
    "                    action = self.main_model(state).max(1)[1].item()\n",
    "            else:\n",
    "                action = random.randrange(self.num_actions)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).view(1, -1)\n",
    "                if torch.cuda.is_available():\n",
    "                    state = state.cuda()\n",
    "                action = self.main_model(state).max(1)[1].item()\n",
    "            \n",
    "        return action\n",
    "        \n",
    "    def decay_epsilon(self, episode):\n",
    "        if self.epsilon > self.min_epsilon:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "       \n",
    "    def _get_model(self):\n",
    "        model = nn.Sequential()\n",
    "        model.add_module('fc1', nn.Linear(self.num_states, 32))\n",
    "        model.add_module('relu1', nn.ReLU())\n",
    "        model.add_module('fc2', nn.Linear(32, 32))\n",
    "        model.add_module('relu2', nn.ReLU())\n",
    "        model.add_module('fc3', nn.Linear(32, self.num_actions))\n",
    "        return model\n",
    "    \n",
    "    def _update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.main_model.state_dict())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDQNAgent(env.observation_space.shape[0], env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCH = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continues_sucess = 0\n",
    "\n",
    "for episode in range(N_EPOCH):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            if step < 195:\n",
    "                reward = -1.0\n",
    "                continues_sucess = 0\n",
    "            else:\n",
    "                reward = 1.0\n",
    "                continues_sucess += 1\n",
    "        else:\n",
    "            reward = 0.0\n",
    "            \n",
    "        agent.train(state, action, next_state, reward, done, episode)\n",
    "        \n",
    "        state = next_state\n",
    "        step += 1\n",
    "        \n",
    "    agent.decay_epsilon(episode)\n",
    "    if continues_sucess >= 10:\n",
    "        break\n",
    "        \n",
    "    print(\"Episode {} Step {}\".format(episode, step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
