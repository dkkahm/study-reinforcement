{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Free Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo Learning\n",
    "\n",
    "Incremental Monte-Carlo Updates\n",
    "\n",
    "- Updates $V(s)$ incrementally after episode $S_1,A_1,R_2,...,S_T$\n",
    "- For each state $S_t$ with return $G_t$\n",
    "\n",
    "\\begin{align}\n",
    "N(S_t) & \\leftarrow N(S_t) +1 \\\\\n",
    "V(S_t) & \\leftarrow V(S_t) + \\frac{1}{N(S_t)}(G_t - V(S_t))\n",
    "\\end{align}\n",
    "\n",
    "- In non-stationary problems, it can be useful to track a running mean, i.e. forget old episodes.\n",
    "\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha (G_t - V(S_t))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal-Difference Learning\n",
    "\n",
    "\n",
    "Simple temporal-difference learning algorithm: TD(0)\n",
    "- Update value $V(S_t)$ toward estimated Return $R_{t+1} + \\gamma V(S_{t+1})$\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha (R_{t+1} + \\gamma V(S_{t+1}) - V(S_t))\n",
    "$$\n",
    "- $R_{t+1} + \\gamma V(S_{t+1})$ is called the TD target\n",
    "- $\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$ is called the TD error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Free Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On-policy learning\n",
    "- \"Learn on the job\"\n",
    "- Learn about policy $\\pi$ from experience sampled from $\\pi$\n",
    "\n",
    "#### Off-policy learning\n",
    "- \"Look over someone's shoulder\"\n",
    "- Learn about policy $\\pi$ from experience sampled from $\\mu$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalised Policy Iteration With Monte-Carlo Evaluation\n",
    "\n",
    "- Policy evaluation: Monte-Carlo policy evaluation, $V = v_\\pi$ ?\n",
    "- Policy improvement: Greedy policy improvement?\n",
    "- Greedy policy improvement over $V(s)$ requires model of MDP\n",
    "$$\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\pi^\\prime(s) = \\argmax\\limits_{a \\in A} \\left( R^a_s + P^a_{s s^\\prime} V(s^\\prime) \\right)\n",
    "$$\n",
    "- Monte-Carlo policy evaluation는 가능하나 Greedy policy improvement은 MDP를 알아야 해서 사용할 수 없다. ($R^a_s$와 $P^a_{s s^\\prime}$를 알아야 한다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-Free Policy Improvement Using Action-Value Function\n",
    "\n",
    "- Greedy policy improvement over $Q(s,a)$ is model-free\n",
    "$$\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\pi^\\prime (s) = \\argmax\\limits_{a \\in A} Q(s,a)\n",
    "$$\n",
    "- $Q(s,a)$를 사용한 Greedy policy improvement는 가능\n",
    "- 그러나, $Q(s,a)$를 사용해서 Greedy policy improvement를 하다 보면, 시도하지 않는 action이 더 좋은 reward를 줄 수도 있다는 정보가 없기 때문에 학습이 정체되게 된다. (stuck)\n",
    "- 따라서, $\\epsilon-\\text{Greedy}$ Exploration 기법을 policy improvement에 적용해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon-\\text{Greedy}$ Exploration\n",
    "- With probability $1-\\epsilon$ choose the greedy action\n",
    "- With propability $\\epsilon$ choose an action at random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo Policy Iteration\n",
    "- Policy evaluation: Monte-Carlo policy evalution, $Q=q_\\pi$\n",
    "- Policy improvement: $\\epsilon-\\text{greedy}$ policy improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo Control\n",
    "- Every episode:\n",
    "- Policy evaluation: Monte-Carlo policy evalution, $Q \\approx q_\\pi$\n",
    "- Policy improvement: $\\epsilon-\\text{greedy}$ policy improvement\n",
    "- 수렴하려면 episode가 진행하면서 $\\epsilon$을 0에 가깝게 줄여 주어야 한다. (GLIE)\n",
    "$$\n",
    "\\epsilon_k = \\frac{1}{k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLIE Monte-Carlo Control\n",
    "- Every episode:\n",
    "- Policy evaluation: Monte-Carlo policy evalution, $Q \\approx q_\\pi$\n",
    "\\begin{align}\n",
    "N(S_t, A_t) & \\leftarrow N(S_t, A_t) + 1\\\\\n",
    "Q(S_t, A_t) & \\leftarrow Q(S_t, A_t) + \\frac{1}{N(S_t, A_t)}(G_t - Q(S_t,A_t)) \\\\\n",
    "& \\frac{1}{N(S_t, A_t)} \\text{can be replaced with } \\alpha\n",
    "\\end{align}\n",
    "- Policy improvement: $\\epsilon-\\text{greedy}$ policy improvement\n",
    "\\begin{align}\n",
    "\\epsilon & \\leftarrow 1 / k \\\\\n",
    "\\pi & \\leftarrow \\epsilon-\\text{greedy}(Q)\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Action-Value Function with Sarsa\n",
    "- GLIE TD Control is Sarsa\n",
    "$$\n",
    "Q(S,A) \\leftarrow Q(S,A) + \\alpha \\left( R + \\gamma Q(S^\\prime, A^\\prime) - Q(S,A) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Control Algorithm\n",
    "\n",
    "$$\n",
    "Q(S,A) \\leftarrow Q(S,A) + \\alpha \\left( R + \\gamma \\max\\limits_{a^\\prime} Q(S^\\prime, a^\\prime) - Q(S,A) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
