{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Armed Bandits\n",
    "- The action-value is the mean reward for action, $a$\n",
    "$$\n",
    "Q(a) = \\mathbb{E}[r|a]\n",
    "$$\n",
    "- The optimal value $V^\\ast$ is\n",
    "$$\n",
    "V^\\ast = Q(a^\\ast) = \\max\\limits_{a \\in A} Q(a)\n",
    "$$\n",
    "- The regret is the opportunity loss for one step\n",
    "$$\n",
    "l_t = \\mathbb{E}[V^\\ast - Q(a_t)]\n",
    "$$\n",
    "- The total regret is the total opportunity loss\n",
    "$$\n",
    "L_t = \\mathbb{E} \\left[ \\sum^t_{\\tau=1} V^\\ast - Q(a_\\tau) \\right]\n",
    "$$\n",
    "- Maximise cumulative reward $\\equiv$ minimise total regret\n",
    "- The count $N_t(a)$ is expected number of selection for action a\n",
    "- The gap $\\bigtriangleup_a$ is the difference in value between action $a$ and the optimal action $a^\\ast, \\bigtriangleup_a=V^\\ast - Q(a)$\n",
    "- Regret is a function gaps and the counts\n",
    "$$\n",
    "\\begin{align}\n",
    "L_t & = \\mathbb{E} \\left[ V^\\ast - Q(a_t) \\right] \\\\\n",
    "& = \\sum_{a \\in A} \\mathbb{E}[ N_t(a)] (V^\\ast - Q(a)) \\\\\n",
    "& = \\sum_{a \\in A} \\mathbb{E}[ N_t(a)] \\bigtriangleup_a \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimistic Initialisation\n",
    "- Initialise $Q(a)$ to high value\n",
    "- Update action value by incremental Monte-Carlo evaluation\n",
    "- Starting with $N(a) > 0$\n",
    "$$\n",
    "\\hat{Q}_t(a_t) =\\hat{Q}_{t-1} + \\frac{1}{N_t(a_t)}(r_t - \\hat{Q}_{t-1})\n",
    "$$\n",
    "- Encourages systemaic exploration early on\n",
    "- But can still lock onto suboptimal action\n",
    "- greedy + optimistic initialisation has linear total regret\n",
    "- $\\epsilon\\text{-greedy}$ + optimistic initialisation has linear total regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decaying $\\epsilon\\text{-greedy}$ Algorithm\n",
    "- Pick a decay schedule for $\\epsilon_1,\\epsilon_2,\\dots$\n",
    "- Consider the following schedule\n",
    "$$\n",
    "\\begin{align}\n",
    "c & > 0 \\\\\n",
    "d & = \\min\\limits_{a|\\bigtriangleup_a > 0} \\bigtriangleup_i \\\\\n",
    "\\epsilon_t & = \\min \\left\\{ 1, \\frac{c|A|}{d^2t} \\right\\}\n",
    "\\end{align}\n",
    "$$\n",
    "- Decaying $\\epsilon_t\\text{-greedy}$ has logarithmic asymptotic total regret!\n",
    "- Unfortunately, schedule requires advance knowledge of gaps\n",
    "- Goal: find an algorithm with sublinear regret for any multi-armed bandit (without knowledge of $R$)\n",
    "- $d$는 1등과 2등의 차이\n",
    "- $t$가 분모에 있어 시간이 갈수록 값이 줄어듬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimism in the Face of Uncertainty\n",
    "- The more uncertain we are about an action-value\n",
    "- The more important it is to explorer that action\n",
    "- It could turn out to be the best action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upper Confidence Bounds\n",
    "- Estimate an upper confidence $\\hat{U}_t(a)$ for each action value\n",
    "- Such that $Q(a) \\leq \\hat{Q}_t(a) + \\hat{U}_t(a)$ with high probability\n",
    "- This depends on the number of times $N(a)$ has been selected\n",
    "  - Small $N_t(a) \\Rightarrow$ large $\\hat{U}_t(a)$ (estimated value is uncertain)\n",
    "  - Large $N_t(a) \\Rightarrow$ small $\\hat{U}_t(a)$ (estimated value is accurate)\n",
    "- Select action maximising Upper Confidence Bound (UCB)\n",
    "$$\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "a_t = \\argmax\\limits_{a \\in A} \\left( \\hat{Q}_t(a) + \\hat{U}_t(a) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCB1\n",
    "$$\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "a_t = \\argmax\\limits_{a \\in A} \\left( Q(a) + \\sqrt{\\frac{2 \\log t}{N_t(a)}} \\right)\n",
    "$$\n",
    "- The UCB algorithm achieves logarithmic asymptotic total regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thompson Sampling\n",
    "- Thompson sampling implements probability matching\n",
    "- Use Bayes law to compute posterior distribution $p[R|h_t]$\n",
    "- Sample a reward distribution $R$ from posterior\n",
    "- Compute action-value function $Q(a) = \\mathbb{E}[R_a]$\n",
    "- Select action maximising value on sample, $\\DeclareMathOperator*{\\argmax}{arg\\,max} a_t = \\argmax\\limits_{a \\in A} Q(a)$\n",
    "- Thompson sampling achieves Lai and Robbins low bound!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information State Search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
