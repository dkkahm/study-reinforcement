{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model based MDP는 Dynamic Programming 으로 풀 수 있다\n",
    "\n",
    "model based MDP는 환경을 알고 있는 경우이다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration으로 model MDP로 풀고자 하는 경우, 문제는 두가지 경우로 나뉜다.\n",
    "\n",
    "1. policy $\\pi(a|s)$를 주고, 주어진 policy를 따랐을 때의 value function을 구하고자 하는 경우, 이 경우를 prediction 문제라고 한다.\n",
    "2. 주어진 MDP의 최적 value function과 최적 policy를 구하고자 하는 경우, 이 경우를 control 문제라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction 문제는 다음의 순서를 반복한다. (Policy Evaluation)\n",
    "\n",
    "1. At each iteration $k + 1$\n",
    "2. For all states $s \\in S$\n",
    "3. Update $v_{k+1}(s)$ from $v_k(s^\\prime)$\n",
    "4. where $s^\\prime$ is a successor state of $s$\n",
    "\n",
    "\\begin{align}\n",
    "v_{k+1}(s) & = \\sum_{a \\in A} \\pi(a|s) \\left( R^a_s + \\gamma \\sum_{s^\\prime \\in S} P^a_{s s^\\prime} v_k(s^\\prime) \\right)\n",
    "\\end{align}\n",
    "\n",
    "### Policy Evaluation과정으로 update된 value function을 greey하게 사용한 policy는 optimal policy로 수렴한다 (Policy Impromvement)\n",
    "\n",
    "policy $\\pi(a|s) = P[A_t=a | S_t=s]$ 이다.\n",
    "1. 다음 식을 사용해서 s에서 모든 a에 대해서 q 값을 구한다.\n",
    "$$\n",
    "q_\\pi(s,a) = R^a_s + \\gamma \\sum_{s^\\prime \\in S} P^a_{s s^\\prime} v_\\pi (s^\\prime)\n",
    "$$\n",
    "2. q 값이 max가 나온 a 만 선택될 수 있도록 정책을 조정한다. max 값이 나오지 않은 다른 a는 확률이 0이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### control 문제의 경우는, 처음에는 random policy를 주고 Policy Evalucation과 Policy Improvement를 번갈아 적용하면 최적 value function과 최적 policy로 수렴하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration은 목표에서 역으로 최적 value function을 계산해 나가는 방법이다.\n",
    "\n",
    "1. 만약 $v_*(s^\\prime)$을 알고 있다면\n",
    "2. $v_*(s)$는 다음의 식을 사용해서 계산할 수 있다. (Bellman optimality backup)\n",
    "\\begin{align}\n",
    "v_*(s) & \\leftarrow \\max\\limits_{a \\in A} \\left(R^a_s + \\gamma \\sum_{s^\\prime \\in S} P^a_{s s^\\prime} v_*(s^\\prime) \\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
