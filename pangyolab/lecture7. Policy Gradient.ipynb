{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Objective Function\n",
    "- Goal: given policy $\\pi_\\theta(s,a)$ with parameter $\\theta$, find best $\\theta$\n",
    "- How to measture the quality of a policy $\\pi_\\theta$ ?\n",
    "  - In episodic environments we can use the start value\n",
    "$$\n",
    "J_1(\\theta) = V^{\\pi_\\theta}(s_1) = \\mathbb{E}_{\\pi_\\theta}[v_1]\n",
    "$$\n",
    "    - 첫번째 state의 value\n",
    "    - 시작 state는 하나 혹은 고정된 분포\n",
    "  - In continious environments we can use the average value\n",
    "$$\n",
    "J_{av}(\\theta) = \\sum_s d^{\\pi_\\theta}(s) V^{\\pi_\\theta}(s)\n",
    "$$\n",
    "    - state에 있을 확률과 state의 value의 곱의 총합\n",
    "  - Or the average reward per time-stamp\n",
    "$$\n",
    "J_{avR}(\\theta) = \\sum_s d^{\\pi_\\theta}(s) \\sum_a \\pi_\\theta(s,a) R^a_s\n",
    "$$\n",
    "    - state에 있을 확률과 state에서 각 action을 했을 때의 reward와 action의 확률(policy)의 곱의 총합\n",
    "  - where $d^{\\pi_\\theta}(s)$ is stationary distribution of Markov chain for $\\pi_\\theta(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Optimisation\n",
    "- Find $\\theta$ that maximises $J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradient\n",
    "$$\n",
    "\\bigtriangleup \\theta = \\alpha \\bigtriangledown_\\theta J(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Function\n",
    "- Assume policy $\\pi_\\theta$ is differentiable whenever it is non-zero\n",
    "- and we know the gradient $\\bigtriangledown_\\theta \\pi_\\theta(s,a)$\n",
    "- Likelihood ratios explits the following identity\n",
    "$$\n",
    "\\begin{align}\n",
    "\\bigtriangledown_\\theta \\pi_\\theta(s,a) & = \\pi_\\theta(s,a) \\frac{\\bigtriangledown_\\theta \\pi_\\theta(s,a)}{\\pi_\\theta(s,a)} \\\\\n",
    "& = \\pi_\\theta(s,a) \\bigtriangledown_\\theta \\log \\pi_\\theta(s,a)\n",
    "\\end{align}\n",
    "$$\n",
    "- The score function is $\\bigtriangledown_\\theta \\log \\pi_\\theta(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Step MDP\n",
    "- Consider a simple class of one-step MDPs\n",
    "  - Starting in state $s \\sim d(s)$\n",
    "  - Terminating after one time-step with reward $r=R_{s,a}$\n",
    "- Use likelihood ratios to compute the policy gradient\n",
    "$$\n",
    "\\begin{align}\n",
    "J(\\theta) & = \\mathbb{E}_{\\pi_\\theta}[r] \\\\\n",
    " & = \\sum_{s \\in S} d(s) \\sum_{a \\in A} \\pi_\\theta(s,a)R_{s,a} \\\\\n",
    "\\bigtriangledown_\\theta J(\\theta) & = \\sum_{s \\in S} d(s) \\sum_{a \\in A} \\pi_\\theta (s,a) \\bigtriangledown_\\theta \\log \\pi_\\theta (s,a) R_{s,a} \\\\\n",
    "& = \\mathbb{E}_{\\pi_\\theta}[\\bigtriangledown_\\theta \\log \\pi_\\theta (s,a) r]\n",
    "\\end{align}\n",
    "$$\n",
    "- 기대값이므로 평균으로 대신할 수 있다.\n",
    "- $\\pi_\\theta (s,a)$는 미분가능한 함수로 선택하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Policy\n",
    "- We will use a softmax policy as a running example\n",
    "- Weight actions using linear combination of features $\\phi(s,a)^T \\theta$\n",
    "- Probability of action is proportional to exponentiated weight\n",
    "$$\n",
    "\\pi_\\theta (s,a) \\propto e^{\\phi(s,a)^T \\theta}\n",
    "$$\n",
    "- The score function is\n",
    "$$\n",
    "\\bigtriangledown_\\theta \\log \\pi_\\theta(s,a) = \\phi(s,a) - \\mathbb{E}_{\\pi_\\theta}[\\phi(s,\\cdot)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Policy\n",
    "- In continious action spaces, a Gaussian policy is natural\n",
    "- Mean is a linear combination of state feature $\\mu(s) = \\phi(s)^T \\theta$\n",
    "- Variance may be fixed $\\sigma^2$, or can be parametrised\n",
    "- Policy is Gaussian, $a \\sim N(\\mu(s), \\sigma^2)$\n",
    "- The score function is\n",
    "$$\n",
    "\\bigtriangledown_\\theta \\log \\pi_\\theta(s,a) = \\frac{(a-\\mu(s))\\phi(s)}{\\sigma^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradient Theorem\n",
    "- 일반화\n",
    "$$\n",
    "\\bigtriangledown_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[\\bigtriangledown_\\theta \\log \\pi_\\theta (s,a) Q^{\\pi_\\theta}(s,a)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo Policy Gradient (REINFORCE)\n",
    "- Update parameters by stochastic gradient ascent\n",
    "- Using policy gradient theorem\n",
    "- Using return $v_t$ as an unbiased sample of $Q^{\\pi_\\theta}(s_t,a_t)$\n",
    "$$\n",
    "\\bigtriangleup \\theta_t = \\alpha \\bigtriangledown_\\theta \\pi_\\theta(s_t,a_t) v_t\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\text{function REINFORCE} \\\\\n",
    "& \\hspace{10mm} \\text{Initialise }\\theta\\text{ arbitrarily} \\\\\n",
    "& \\hspace{10mm} \\text{for each episode }\\{s_1, a_1, r_2, \\dots, s_{T-1}, a_{T-1}, r_T\\} \\sim \\pi_\\theta\\text{ do}\\\\\n",
    "& \\hspace{20mm} \\text{for }t = 1\\text{ to }T - 1\\text{ do}\\\\\n",
    "& \\hspace{30mm} \\theta \\leftarrow \\theta + \\alpha \\bigtriangledown_\\theta \\log \\pi_\\theta(s_t,a_t)v_t\\\\\n",
    "& \\hspace{20mm} \\text{end for }\\\\\n",
    "& \\hspace{10mm} \\text{end for }\\\\\n",
    "& \\hspace{10mm} \\text{return }\\theta\\\\\n",
    "& \\text{end function}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Variace Using a Critic\n",
    "- We use a critic to estimate the action-value function\n",
    "$$\n",
    "Q_w(s,a) \\approx Q^{\\pi_\\theta}(s,a)\n",
    "$$\n",
    "- Actor-critic algorithm maintain two sets of parameters\n",
    "  - Critic: Updates action-value function parameters $w$\n",
    "  - Actor: Updates policy parameters $\\theta$, in direction suggested by critic\n",
    "- Actor-critic algorithms follow an approximate policy gradient\n",
    "$$\n",
    "\\begin{align}\n",
    "\\bigtriangledown_\\theta J(\\theta) & \\approx \\mathbb{E}_{\\pi_\\theta}[\\bigtriangledown_\\theta \\log \\pi_\\theta (s,a) Q_w(s,a)] \\\\\n",
    "\\bigtriangleup \\theta & = \\alpha \\bigtriangledown_\\theta \\log \\pi_\\theta(s,a) Q_w(s,a)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action-Value Actor-Critic\n",
    "- Simple actor-critic algorithm based on action-value critic\n",
    "- Using linear value fn approx. $Q_w(s,a) = \\phi(s,a)^T w$\n",
    "  - Critic: Updates $w$ by linear TD(0)\n",
    "  - Actor: Updates $\\theta$ by policy gradient\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\text{function QAC}\\\\\n",
    "& \\hspace{10mm}\\text{Initialise s, }\\theta \\\\\n",
    "& \\hspace{10mm}\\text{Sample }a \\sim \\pi_\\theta \\\\\n",
    "& \\hspace{10mm}\\text{for each step do} \\\\\n",
    "& \\hspace{20mm}\\text{Sample reward }r=R^a_s\\text{; sample transition }s^\\prime \\sim P^a_{s,\\cdot} \\\\\n",
    "& \\hspace{20mm}\\text{Sample action }a^\\prime \\sim \\pi_\\theta(s^\\prime, a^\\prime) \\\\\n",
    "& \\hspace{20mm}\\delta = r + \\gamma Q_w(s^\\prime, a^\\prime) - Q_w(s, a) \\\\\n",
    "& \\hspace{20mm}\\theta = \\theta + \\alpha \\bigtriangledown_\\theta \\log \\pi_\\theta(s,a)Q_w(s,a) \\\\\n",
    "& \\hspace{20mm}w \\leftarrow w + \\beta \\delta \\phi(s,a) \\\\\n",
    "& \\hspace{20mm}a \\leftarrow a^\\prime, s \\leftarrow s^\\prime \\\\\n",
    "& \\hspace{10mm}\\text{end for} \\\\\n",
    "& \\text{end function}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Variance Using a Baseline\n",
    "- We substract a baseline function $B(s)$ from the policy gradient\n",
    "- This can reduce variance, without changing expectation\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{\\pi_\\theta}[\\bigtriangledown_\\theta \\log \\pi_\\theta(s,a) B(s)] & = \\sum_{s \\in S} d^{\\pi_\\theta}(s) \\sum_a \\bigtriangledown_\\theta \\pi_\\theta(s,a) B(s) \\\\\n",
    "& = \\sum_{s \\in S} d^{\\pi_\\theta}(s)  B(s) \\bigtriangledown_\\theta \\sum_a \\pi_\\theta(s,a) \\\\\n",
    "& = 0\n",
    "\\end{align}\n",
    "$$\n",
    "- A good baseline is the state value function $B(s) = V^{\\pi_\\theta}(s)$\n",
    "- So we can rewrite the policy gradient using advantage function $A^{\\pi_\\theta}(s,a)$\n",
    "$$\n",
    "\\begin{align}\n",
    "A^{\\pi_\\theta}(s,a) & = Q^{\\pi_\\theta}(s,a) - V^{\\pi_\\theta}(s) \\\\\n",
    "\\bigtriangledown_\\theta J(\\theta) & = \\mathbb{E}_{\\pi_\\theta}[\\bigtriangledown_\\theta \\log \\pi_\\theta(s,a) A^{\\pi_\\theta}(s,a)]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the Advantage Function\n",
    "- Using two function approximators and two parameter vectors\n",
    "$$\n",
    "\\begin{align}\n",
    "V_v(s) & \\approx V^{\\pi_\\theta}(s) \\\\\n",
    "Q_w(s,a) & \\approx Q^{\\pi_\\theta}(s,a) \\\\\n",
    "A(s, a) & = Q_w(s,a) - V_v(s)\n",
    "\\end{align}\n",
    "$$\n",
    "- Critic을 위해서 $v, w$의 두 파라메터가 필요하다. 그러나 ...\n",
    "- For the true value function $V^{\\pi_\\theta}(s)$, the TD error $\\delta^{\\pi_\\theta}$\n",
    "$$\n",
    "\\delta^{\\pi_\\theta} = r + \\gamma V^{\\pi_\\theta}(s^\\prime) - V^{\\pi_\\theta}(s)\n",
    "$$\n",
    "- is unbiased estimate of the advantage function\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{\\pi_\\theta}[\\delta^{\\pi_\\theta}|s,a] & = \\mathbb{E}_{\\pi_\\theta}[r + \\gamma V^{\\pi_\\theta}(s^\\prime)|s,a] - V^{\\pi_\\theta}(s) \\\\\n",
    "& = Q^{\\pi_\\theta}(s,a) - V^{\\pi_\\theta}(s) \\\\\n",
    "& = A^{\\pi_\\theta}(s,a)\n",
    "\\end{align}\n",
    "$$\n",
    "- So we can use the TD error to compute the policy gradient\n",
    "$$\n",
    "\\bigtriangledown_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[\\bigtriangledown_\\theta \\log \\pi_\\theta(s,a) \\delta^{\\pi_\\theta}]\n",
    "$$\n",
    "- In practice we can use an approximate TD error\n",
    "$$\n",
    "\\delta_v = r + \\gamma V_v(s^\\prime)-V_v(s)\n",
    "$$\n",
    "- This approach only requires one set of critic parameter $v$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
