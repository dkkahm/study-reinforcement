{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Model\n",
    "- Goal: estimate model $M_\\eta$ from experience $\\{ S_1,A_1,R_2, \\dots, S_T \\}$\n",
    "- This is a supervised learning problem\n",
    "$$\n",
    "S_1,A_1 \\rightarrow R_2, S_2 \\\\\n",
    "S_2,A_2 \\rightarrow R_3, S_3 \\\\\n",
    "\\vdots \\\\\n",
    "S_{T-1},A_{T-1} \\rightarrow R_T, S_T\n",
    "$$\n",
    "- Learning $s,a \\rightarrow r$ is regression problem\n",
    "- Learning $s,a \\rightarrow s^\\prime$ is density estimation problem\n",
    "- Pick loss function, e.g. mean-square error, KL divergence\n",
    "- Find parameter $\\eta$ that minimise emprical loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Lookup Model\n",
    "- Model is an explicit MDP, $\\hat{P}$, $\\hat{R}$\n",
    "- Count visits $N(s,a)$ to each state action pair\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{P}^a_{s,s^\\prime} & = \\frac{1}{N(s,a)}\\sum^T_{t=1} \\boldsymbol{1}(S_t,A_t,S_{t+1}=s,a,s^\\prime) \\\\\n",
    "\\hat{R}^a_s & = \\frac{1}{N(s,a)}\\sum^T_{t=1}\\boldsymbol{1}(S_t,A_t=s,a)R_t\n",
    "\\end{align}\n",
    "$$\n",
    "- Alternatively\n",
    "  -  At each time-stamp $t$, record experience tuple $\\langle S_t,A_t,R_{t+1},S_{t+1} \\rangle$\n",
    "  - To sample model, randomly pick tuple matching $\\langle s, a, \\cdot, \\cdot \\rangle$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dyna\n",
    "- Learn a model from real experience\n",
    "- Learn and play value function (and/or policy) from real and simulated experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dyna-Q Algorithm\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\text{Initialize }Q(s,a)\\text{ and }Model(s,a)\\text{ for all }s \\in S\\text{ and }a \\in A(s) \\\\\n",
    "& \\text{Do forever:} \\\\\n",
    "& \\hspace{10mm} \\text{(a) }S \\leftarrow\\text{ current (nonterminal) state} \\\\\n",
    "& \\hspace{10mm} \\text{(b) }A \\leftarrow \\epsilon\\text{-greedy}(S,Q) \\\\\n",
    "& \\hspace{10mm} \\text{(c) Execute action }A\\text{; observe resultant reward, }R\\text{, and state, }S^\\prime \\\\\n",
    "& \\hspace{10mm} \\text{(d) }Q(S,A) \\leftarrow Q(S,A) + \\alpha [R + \\gamma \\max_a Q(S^\\prime,a) - Q(S,A)] \\\\\n",
    "& \\hspace{10mm} \\text{(e) }Model(S,A) \\leftarrow R,S^\\prime\\text{ (assuming deterministic environment)} \\\\\n",
    "& \\hspace{10mm} \\text{(f) Releat }n\\text{ times} \\\\\n",
    "& \\hspace{20mm} S \\leftarrow \\text{random previously observed state} \\\\\n",
    "& \\hspace{20mm} A \\leftarrow \\text{random action previously taken in }S \\\\\n",
    "& \\hspace{20mm} R,S^\\prime \\leftarrow Model(S,A) \\\\\n",
    "& \\hspace{20mm} Q(S,A) \\leftarrow Q(S,A) + \\alpha [R + \\gamma \\max_a Q(S^\\prime,a) - Q(S,A)]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Monte-Carlo Search\n",
    "- Given a model $M_v$ and a simulation policy $\\pi$\n",
    "- For each action $a \\in A$\n",
    "  - Simulate $K$ episodes from current (real) state $s_t$\n",
    "  $$\n",
    "  \\{s_t,a,R^k_{t+1},S^k_{t+1},A^k_{t+1},\\dots,S^k_T\\}^K_{k=1} \\sim M_v,\\pi\n",
    "  $$\n",
    "  - Evaluate actions by mean return (Monte-Carlo evaluation)\n",
    "  $$\n",
    "  Q(s_t,a_t) = \\frac{1}{K} \\sum^K_{k=1} G_t \\stackrel{P}\\rightarrow q_\\pi(s_t,a)\n",
    "  $$\n",
    "- Select current (real) action with maximum value\n",
    "$$\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "a_t = \\argmax\\limits_{a \\in A} Q(s_t,a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo Tree Search (Evaluation)\n",
    "- Given a model $M_v$\n",
    "- Simulate $K$ episodes from current state $s_t$ using current simulation policy $\\pi$\n",
    "$$\n",
    "\\{ s_t, A^k_t, R^k_{t+1}, S^k_{t+1},\\dots,S^k_T\\}^K_{k=1} \\sim M_v,\\pi\n",
    "$$\n",
    "- Build a search tree containing visited states and actions\n",
    "- Evaluate states $Q(s,a)$ by mean return of episodes from $s, a$\n",
    "$$\n",
    "Q(s,a) = \\frac{1}{N(s,a)}\\sum^K_{k=1}\\sum^T_{u=t}\\boldsymbol{1}(S_u,A_u=s,a)G_u\\stackrel{P}\\rightarrow q_\\pi(s,a)\n",
    "$$\n",
    "- After search is finished, select current (real) action with maxium value in search tree\n",
    "$$\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "a_t = \\argmax\\limits_{a \\in A} Q(s_t,a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo Tree Search (Simulation)\n",
    "- In MCTS, the simulation policy $\\pi$ improves\n",
    "- Each simulation cosists of two phase (in-tree, out-of-tree)\n",
    "  - Tree policy (improves): pick action to maximise $Q(s, a)$\n",
    "  - Default policy (fixed): pick action randomly\n",
    "- Repeat (each simulation)\n",
    "  - Evaluate states $Q(S,A)$ by Monte-Carlo evaluation\n",
    "  - Improve tree policy, e.g. by $\\epsilon\\text{-greedy}(Q)$\n",
    "- Monte-Carlo control applied to simulated experience\n",
    "- Converges on the optimal search tree, $Q(S,A) \\rightarrow q_\\ast(S,A)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
