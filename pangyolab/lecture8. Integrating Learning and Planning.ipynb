{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Model\n",
    "- Goal: estimate model $M_\\eta$ from experience $\\{ S_1,A_1,R_2, \\dots, S_T \\}$\n",
    "- This is a supervised learning problem\n",
    "$$\n",
    "S_1,A_1 \\rightarrow R_2, S_2 \\\\\n",
    "S_2,A_2 \\rightarrow R_3, S_3 \\\\\n",
    "\\vdots \\\\\n",
    "S_{T-1},A_{T-1} \\rightarrow R_T, S_T\n",
    "$$\n",
    "- Learning $s,a \\rightarrow r$ is regression problem\n",
    "- Learning $s,a \\rightarrow s^\\prime$ is density estimation problem\n",
    "- Pick loss function, e.g. mean-square error, KL divergence\n",
    "- Find parameter $\\eta$ that minimise emprical loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Lookup Model\n",
    "- Model is an explicit MDP, $\\hat{P}$, $\\hat{R}$\n",
    "- Count visits $N(s,a)$ to each state action pair\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{P}^a_{s,s^\\prime} & = \\frac{1}{N(s,a)}\\sum^T_{t=1} \\boldsymbol{1}(S_t,A_t,S_{t+1}=s,a,s^\\prime) \\\\\n",
    "\\hat{R}^a_s & = \\frac{1}{N(s,a)}\\sum^T_{t=1}\\boldsymbol{1}(S_t,A_t=s,a)R_t\n",
    "\\end{align}\n",
    "$$\n",
    "- Alternatively\n",
    "  -  At each time-stamp $t$, record experience tuple $\\langle S_t,A_t,R_{t+1},S_{t+1} \\rangle$\n",
    "  - To sample model, randomly pick tuple matching $\\langle s, a, \\cdot, \\cdot \\rangle$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dyna\n",
    "- Learn a model from real experience\n",
    "- Learn and play value function (and/or policy) from real and simulated experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dyna-Q Algorithm\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\text{Initialize }Q(s,a)\\text{ and }Model(s,a)\\text{ for all }s \\in S\\text{ and }a \\in A(s) \\\\\n",
    "& \\text{Do forever:} \\\\\n",
    "& \\hspace{10mm} \\text{(a) }S \\leftarrow\\text{ current (nonterminal) state} \\\\\n",
    "& \\hspace{10mm} \\text{(b) }A \\leftarrow \\epsilon\\text{-greedy}(S,Q) \\\\\n",
    "& \\hspace{10mm} \\text{(c) Execute action }A\\text{; observe resultant reward, }R\\text{, and state, }S^\\prime \\\\\n",
    "& \\hspace{10mm} \\text{(d) }Q(S,A) \\leftarrow Q(S,A) + \\alpha [R + \\gamma \\max_a Q(S^\\prime,a) - Q(S,A)] \\\\\n",
    "& \\hspace{10mm} \\text{(e) }Model(S,A) \\leftarrow R,S^\\prime\\text{ (assuming deterministic environment)} \\\\\n",
    "& \\hspace{10mm} \\text{(f) Releat }n\\text{ times} \\\\\n",
    "& \\hspace{20mm} S \\leftarrow \\text{random previously observed state} \\\\\n",
    "& \\hspace{20mm} A \\leftarrow \\text{random action previously taken in }S \\\\\n",
    "& \\hspace{20mm} R,S^\\prime \\leftarrow Model(S,A) \\\\\n",
    "& \\hspace{20mm} Q(S,A) \\leftarrow Q(S,A) + \\alpha [R + \\gamma \\max_a Q(S^\\prime,a) - Q(S,A)]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
